---
title: "NLP - Spanish Debates"
author: "Alejandro Alija, PhD."
date: "18/01/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Environment

Para ejecutar este proyecto se ha utilizado una imágen Docker de RStudio. En particular rocker/rstudio. Para más información: https://hub.docker.com/r/rocker/rstudio 

## The data

```{r}
# Dependencies
first_run <- T
if(first_run == T) {
  install.packages(c("udpipe", "lattice", "ggplot2","igraph", "ggraph", "textrank","data.table", "wordcloud", "dplyr", "magrittr"), quiet = T, dependencies = T )
}

library(udpipe)
library(lattice)
library(ggplot2)
library(igraph)
library(ggraph)
library(textrank)
library(wordcloud)
library(magrittr)
library(dplyr)




```


```{r etl}
#importing data from comments and debates

debates_comentarios <- read.csv("your_path_here", sep=";", comment.char="#", stringsAsFactors=FALSE)

# my path is:
#debates_comentarios <- read.csv("~/kitematic/REDES_Report3/comments (1).csv", sep=";", comment.char="#", stringsAsFactors=FALSE)

# Idem for debates
debates <- read.csv("your_path_here", sep=";", comment.char="#", stringsAsFactors=FALSE)


# my path is:
#debates <- read.csv("~/kitematic/REDES_Report3/debates.csv", sep=";", comment.char="#", stringsAsFactors=FALSE)

#Looking at the relation fields
relation <- "commentable_id (commentarios) related to id (debates)"

#Subsetting the comments data frame
# just get the 6 first debates and discarding Polls and Proposals

debates_comentarios_filtered <- subset(debates_comentarios, commentable_id < 100 & commentable_type == "Debate", select=c(id,commentable_id, commentable_type, body))
debates_filtered <- subset(debates, id < 100, select=c(id, title, description))
```

## Natural Language Processing - First Steps


```{r udpipe, echo=FALSE}
#Lets start the analysis

data(debates_comentarios_filtered)

ud_model <- udpipe_download_model(language = "spanish")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = debates_comentarios_filtered$body, doc_id =debates_comentarios_filtered$commentable_id)
x <- as.data.frame(x)
```

The resulting data.frame has a field called upos which is the Universal Parts of Speech tag and also a field called lemma which is the root form of each token in the text. These 2 fields give us a broad range of analytical possibilities.

### Basic frequency statistics

In most languages, nouns (NOUN) are the most common types of words, next to verbs (VERB) and these are the most relevant for analytical purposes, next to the adjectives (ADJ) and proper nouns (PROPN). 
For a detailed list of all POS tags: visit http://universaldependencies.org/u/pos/index.html.

```{r upos}
stats <- txt_freq(x$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "grey", 
         main = "UPOS (Universal Parts of Speech)\n frequency of occurrence", 
         xlab = "Freq")
```

Parts of Speech tags are really interesting to extract easily the words you like to plot. You really don't need stopwords for doing this, just select nouns / verbs or adjectives and you have already the most relevant parts for basic frequency analysis.

```{r nouns}
## NOUNS
stats <- subset(x, upos %in% c("NOUN")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "grey", 
         main = "Most occurring nouns", xlab = "Freq")
```


```{r adjetives}
## ADJECTIVES
stats <- subset(x, upos %in% c("ADJ")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "grey", 
         main = "Most occurring adjectives", xlab = "Freq")
```

## Finding keywords

Frequency statistics of words are nice but most of the time, you are getting stuck in words which only make sense in combination with other words. Hence you want to find keywords which are a combination of words.

Currently, this R package provides 3 methods to identify keywords in text

- RAKE (Rapid Automatic Keyword Extraction)
- Collocation ordering using Pointwise Mutual Information
- Parts of Speech phrase sequence detection

```{r rake}
## Using RAKE
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id", 
                       relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "grey", 
         main = "Keywords identified by RAKE", 
         xlab = "Rake")
```

```{r}
## Using Pointwise Mutual Information Collocations
x$word <- tolower(x$token)
stats <- keywords_collocation(x = x, term = "word", group = "doc_id")
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ pmi, data = head(subset(stats, freq > 3), 20), col = "grey", 
         main = "Keywords identified by PMI Collocation", 
         xlab = "PMI (Pointwise Mutual Information)")
```

```{r}
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)
stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "grey", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")
```

## Co-occurrences

Co-occurrences allow to see how words are used either in the same sentence or next to each other. This R package make creating co-occurrence graphs using the relevant Parts of Speech tags as easy as possible

### Nouns / adjectives used in same sentence

In this example we look how many times nouns and adjectives are used in the same sentence.


```{r}
y <- subset(x, upos %in% c("NOUN", "ADJ"))
y$doc_id <- as.integer(y$doc_id)
#cooc <- cooccurrence(y, term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
head(cooc)
```


The result can be easily visualised using the ggraph R package which can visualise the word network.


```{r}

wordnetwork <- head(cooc, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  theme(legend.position = "none") +
  labs(title = "Cooccurrences within sentence", subtitle = "Nouns & Adjective")
```

### Nouns / adjectives which follow one another


If you are interested in visualising which words follow one another. This can be done by calculating word cooccurrences of a specific Parts of Speech type which follow one another where you can specify how far away you want to look regarding 'following one another' (in the example below we indicate skipgram = 1 which means look to the next word and the word after that).

```{r}
#cooc2 <- cooccurrence(x$lemma, relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 1)
head(cooc)
```


```{r}
wordnetwork <- head(cooc2, 15)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc)) +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  labs(title = "Words following one another", subtitle = "Nouns & Adjective")
```


### Wordcloud

```{r}
stats <- textrank_keywords(x$lemma, 
                          relevant = x$upos %in% c("NOUN", "ADJ"), 
                          ngram_max = 4, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 5)
wordcloud(words = stats$keyword, freq = stats$freq, min.freq = 5, max.words = 100,
          random.order = FALSE, colors = c("#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02"))
```





```{r, echo=F}

sentiments <- txt_sentiment(x, 
                        term = "lemma",
                        polarity_terms = data.frame(term = c("molesto", "gusta", "doloroso", "bueno", "mejor", "buena", "dificil", "facil"), 
                                                    polarity = c(-1, 1, -1, 1, 1, 1,-1, 1)), 
                        polarity_negators = c("no", "tampoco", "nada"),
                        polarity_amplifiers = c("bonito", "mucho", "de verdad", "lo que"), 
                        polarity_deamplifiers = c("ligeramente", "algo"),
                        constrain = TRUE, n_before = 4,
                        n_after = 2, amplifier_weight = .8)

sentiments <- sentiments$data
```


